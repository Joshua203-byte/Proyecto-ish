# ═══════════════════════════════════════════════════════════════════════════════
# Home-GPU-Cloud Standard ML Image
# Version: standard-v2 (DGX Spark / Grace Blackwell ARM64)
# ═══════════════════════════════════════════════════════════════════════════════
#
# NVIDIA DGX Spark with Grace Blackwell (GB10) Superchip:
# - Architecture: ARM64 (aarch64)
# - Memory: 128GB Unified LPDDR5X (shared CPU/GPU)
# - GPU: NVIDIA Blackwell (Integrated)
# - OS: Ubuntu 22.04 LTS (ARM64)
#
# Build: docker build --platform linux/arm64 -t home-gpu-cloud:standard-v2 -f Dockerfile.standard .
# ═══════════════════════════════════════════════════════════════════════════════

# ───────────────────────────────────────────────────────────────────────────────
# STAGE 1: Base CUDA Runtime (ARM64 / Grace Blackwell)
# ───────────────────────────────────────────────────────────────────────────────
# Use NVIDIA L4T (Linux for Tegra) or NGC containers for ARM64
# For DGX Spark with Blackwell, use the latest CUDA 12.x ARM64 image
FROM nvcr.io/nvidia/cuda:12.4.0-runtime-ubuntu22.04 AS base

# Verify architecture at build time
RUN echo "Building for architecture: $(uname -m)" && \
    if [ "$(uname -m)" != "aarch64" ]; then \
    echo "WARNING: Not building on ARM64 architecture!"; \
    fi

# Prevent interactive prompts during apt install
ENV DEBIAN_FRONTEND=noninteractive

# NVIDIA Container Toolkit environment variables
ENV NVIDIA_VISIBLE_DEVICES=all
ENV NVIDIA_DRIVER_CAPABILITIES=compute,utility

# Python environment
ENV PYTHONUNBUFFERED=1
ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONIOENCODING=UTF-8
ENV PIP_NO_CACHE_DIR=1
ENV PIP_DISABLE_PIP_VERSION_CHECK=1

# ───────────────────────────────────────────────────────────────────────────────
# STAGE 2: System Dependencies (ARM64)
# ───────────────────────────────────────────────────────────────────────────────
RUN apt-get update && apt-get install -y --no-install-recommends \
    # Python (ARM64 native)
    python3.10 \
    python3.10-venv \
    python3.10-dev \
    python3-pip \
    # Build tools (needed for some ARM64 wheel compilation)
    build-essential \
    cmake \
    ninja-build \
    # System utilities
    git \
    curl \
    wget \
    procps \
    # Process management (for signal handling)
    tini \
    # Compression tools (for datasets)
    unzip \
    tar \
    gzip \
    # Locale support
    locales \
    # HDF5 for ARM64 (h5py)
    libhdf5-dev \
    # OpenCV dependencies for ARM64
    libgl1-mesa-glx \
    libglib2.0-0 \
    && locale-gen en_US.UTF-8 \
    # Cleanup apt cache to reduce image size
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*

# Set locale
ENV LANG=en_US.UTF-8
ENV LANGUAGE=en_US:en
ENV LC_ALL=en_US.UTF-8

# Make python3 the default
RUN update-alternatives --install /usr/bin/python python /usr/bin/python3.10 1 \
    && update-alternatives --install /usr/bin/pip pip /usr/bin/pip3 1

# ───────────────────────────────────────────────────────────────────────────────
# STAGE 3: Python ML Dependencies (ARM64 + Blackwell)
# ───────────────────────────────────────────────────────────────────────────────
WORKDIR /app

# Copy requirements first (Docker layer caching)
COPY requirements.standard.txt /tmp/requirements.txt

# Install PyTorch with CUDA support for ARM64 + ML libraries
# Using NVIDIA's PyPI index for ARM64-optimized wheels
RUN pip install --upgrade pip setuptools wheel \
    && pip install -r /tmp/requirements.txt \
    # Cleanup pip cache
    && pip cache purge \
    && rm -rf ~/.cache/pip /tmp/*

# ───────────────────────────────────────────────────────────────────────────────
# STAGE 4: Job Wrapper Setup
# ───────────────────────────────────────────────────────────────────────────────
# Copy the job wrapper script (entrypoint)
COPY job_wrapper.py /opt/home-gpu-cloud/job_wrapper.py
RUN chmod +x /opt/home-gpu-cloud/job_wrapper.py

# Create non-root user for security (but allow GPU access)
RUN useradd -m -s /bin/bash -u 1000 gpuuser \
    && chown -R gpuuser:gpuuser /app

# ───────────────────────────────────────────────────────────────────────────────
# STAGE 5: Final Configuration
# ───────────────────────────────────────────────────────────────────────────────
# Working directory for user scripts
WORKDIR /workspace

# Create standard directories
RUN mkdir -p /workspace/input /workspace/output \
    && chown -R gpuuser:gpuuser /workspace

# Switch to non-root user
USER gpuuser

# Use tini as init process (proper signal handling)
ENTRYPOINT ["/usr/bin/tini", "--", "python", "/opt/home-gpu-cloud/job_wrapper.py"]

# Default command shows help
CMD ["--help"]

# ───────────────────────────────────────────────────────────────────────────────
# Metadata
# ───────────────────────────────────────────────────────────────────────────────
LABEL maintainer="Home-GPU-Cloud Team"
LABEL version="standard-v2"
LABEL description="Standard ML environment for DGX Spark (Grace Blackwell ARM64)"
LABEL architecture="linux/arm64"
LABEL cuda.version="12.4"
LABEL python.version="3.10"
LABEL pytorch.version="2.5.0"
LABEL hardware="NVIDIA DGX Spark (GB10)"
